{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nearest neighbor for handwritten digit recognition\n",
    "\n",
    "In this notebook we will build a classifier that takes an image of a handwritten digit and outputs a label 0-9. We will look at a particularly simple strategy for this problem known as the **nearest neighbor classifier**.\n",
    "\n",
    "To run this notebook you should have the following Python packages installed:\n",
    "* `numpy`\n",
    "* `matplotlib`\n",
    "* `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The MNIST dataset\n",
    "\n",
    "`MNIST` is a classic dataset in machine learning, consisting of 28x28 gray-scale images handwritten digits. The original training set contains 60,000 examples and the test set contains 10,000 examples. In this notebook we will be working with a subset of this data: a training set of 7,500 examples and a test set of 1,000 examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import time\n",
    "\n",
    "## Load the training set\n",
    "train_data = np.load('MNIST/train_data.npy')\n",
    "train_labels = np.load('MNIST/train_labels.npy')\n",
    "\n",
    "## Load the testing set\n",
    "test_data = np.load('MNIST/test_data.npy')\n",
    "test_labels = np.load('MNIST/test_labels.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Print out their dimensions\n",
    "print(\"Training dataset dimensions: \", np.shape(train_data))\n",
    "print(\"Number of training labels: \", len(train_labels))\n",
    "print(\"Testing dataset dimensions: \", np.shape(test_data))\n",
    "print(\"Number of testing labels: \", len(test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute the number of examples of each digit\n",
    "train_digits, train_counts = np.unique(train_labels, return_counts=True)\n",
    "print(\"Training set distribution:\")\n",
    "print(dict(zip(train_digits, train_counts)))\n",
    "\n",
    "test_digits, test_counts = np.unique(test_labels, return_counts=True)\n",
    "print(\"Test set distribution:\")\n",
    "print(dict(zip(test_digits, test_counts)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Visualizing the data\n",
    "Each data point is stored as 784-dimensional vector. To visualize a data point, we first reshape it to a 28x28 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define a function that displays a digit given its vector representation\n",
    "def show_digit(x):\n",
    "    plt.axis('off')\n",
    "    plt.imshow(x.reshape((28,28)), cmap=plt.cm.gray)\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "## Define a function that takes an index into a particular data set (\"train\" or \"test\") and displays that image.\n",
    "def vis_image(index, dataset=\"train\"):\n",
    "    if(dataset==\"train\"): \n",
    "        show_digit(train_data[index,])\n",
    "        label = train_labels[index]\n",
    "    else:\n",
    "        show_digit(test_data[index,])\n",
    "        label = test_labels[index]\n",
    "    print(\"Label \" + str(label))\n",
    "    return\n",
    "\n",
    "## View the first data point in the training set\n",
    "vis_image(0, \"train\")\n",
    "\n",
    "## Now view the first data point in the test set\n",
    "vis_image(0, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## 3. Squared Euclidean distance\n",
    "\n",
    "To compute nearest neighbors in our data set, we need to first be able to compute distances between data points. A natural distance function is _Euclidean distance_: for two vectors $x, y \\in \\mathbb{R}^d$, their Euclidean distance is defined as \n",
    "$$\\|x - y\\| = \\sqrt{\\sum_{i=1}^d (x_i - y_i)^2}.$$\n",
    "Often we omit the square root, and simply compute _squared Euclidean distance_:\n",
    "$$\\|x - y\\|^2 = \\sum_{i=1}^d (x_i - y_i)^2.$$\n",
    "For the purposes of nearest neighbor computations, the two are equivalent: for three vectors $x, y, z \\in \\mathbb{R}^d$, we have $\\|x - y\\| \\leq \\|x - z\\|$ if and only if $\\|x - y\\|^2 \\leq \\|x - z\\|^2$.\n",
    "\n",
    "Now we just need to be able to compute squared Euclidean distance. The following function does so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Computes squared Euclidean distance between two vectors.\n",
    "def squared_dist(x,y):\n",
    "    return np.sum(np.square(x-y))\n",
    "\n",
    "## Compute distance between a seven and a one in our training set.\n",
    "print(\"Distance from 7 to 1: \", squared_dist(train_data[4,],train_data[5,]))\n",
    "\n",
    "## Compute distance between a seven and a two in our training set.\n",
    "print(\"Distance from 7 to 2: \", squared_dist(train_data[4,],train_data[1,]))\n",
    "\n",
    "## Compute distance between two seven's in our training set.\n",
    "print(\"Distance from 7 to 7: \", squared_dist(train_data[4,],train_data[7,]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Computing nearest neighbors\n",
    "\n",
    "Now that we have a distance function defined, we can now turn to nearest neighbor classification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Takes a vector x and returns the index of its nearest neighbor in train_data\n",
    "def find_NN(x):\n",
    "    # Compute distances from x to every row in train_data\n",
    "    distances = [squared_dist(x,train_data[i,]) for i in range(len(train_labels))]\n",
    "    # Get the index of the smallest distance\n",
    "    return np.argmin(distances)\n",
    "\n",
    "## Takes a vector x and returns the class of its nearest neighbor in train_data\n",
    "def NN_classifier(x):\n",
    "    # Get the index of the the nearest neighbor\n",
    "    index = find_NN(x)\n",
    "    # Return its class\n",
    "    return train_labels[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A success case:\n",
    "print(\"A success case:\")\n",
    "print(\"NN classification: \", NN_classifier(test_data[0,]))\n",
    "print(\"True label: \", test_labels[0])\n",
    "print(\"The test image:\")\n",
    "vis_image(0, \"test\")\n",
    "print(\"The corresponding nearest neighbor image:\")\n",
    "vis_image(find_NN(test_data[0,]), \"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A failure case:\n",
    "print(\"A failure case:\")\n",
    "print(\"NN classification: \", NN_classifier(test_data[39,]))\n",
    "print(\"True label: \", test_labels[39])\n",
    "print(\"The test image:\")\n",
    "vis_image(39, \"test\")\n",
    "print(\"The corresponding nearest neighbor image:\")\n",
    "vis_image(find_NN(test_data[39,]), \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. For you to try\n",
    "The above two examples show the results of the NN classifier on test points number 0 and 39.\n",
    "\n",
    "Now try test point number 100.\n",
    "* What is the index of its nearest neighbor in the training set? _Record the answer: you will enter it as part of this week's assignment._\n",
    "* Display both the test point and its nearest neighbor.\n",
    "* What label is predicted? Is this the correct label?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Processing the full test set\n",
    "\n",
    "Now let's apply our nearest neighbor classifier over the full data set. \n",
    "\n",
    "Note that to classify each test point, our code takes a full pass over each of the 7500 training examples. Thus we should not expect testing to be very fast. The following code takes about 100-150 seconds on 2.6 GHz Intel Core i5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict on each test data point (and time it!)\n",
    "t_before = time.time()\n",
    "test_predictions = [NN_classifier(test_data[i,]) for i in range(len(test_labels))]\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute the error\n",
    "err_positions = np.not_equal(test_predictions, test_labels)\n",
    "error = float(np.sum(err_positions))/len(test_labels)\n",
    "\n",
    "print(\"Error of nearest neighbor classifier: \", error)\n",
    "print(\"Classification time (seconds): \", t_after - t_before)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Faster nearest neighbor methods\n",
    "\n",
    "Performing nearest neighbor classification in the way we have presented requires a full pass through the training set in order to classify a single point. If there are $N$ training points in $\\mathbb{R}^d$, this takes $O(N d)$ time.\n",
    "\n",
    "Fortunately, there are faster methods to perform nearest neighbor look up if we are willing to spend some time preprocessing the training set. `scikit-learn` has fast implementations of two useful nearest neighbor data structures: the _ball tree_ and the _k-d tree_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import BallTree\n",
    "\n",
    "## Build nearest neighbor structure on training data\n",
    "t_before = time.time()\n",
    "ball_tree = BallTree(train_data)\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute training time\n",
    "t_training = t_after - t_before\n",
    "print(\"Time to build data structure (seconds): \", t_training)\n",
    "\n",
    "## Get nearest neighbor predictions on testing data\n",
    "t_before = time.time()\n",
    "test_neighbors = np.squeeze(ball_tree.query(test_data, k=1, return_distance=False))\n",
    "ball_tree_predictions = train_labels[test_neighbors]\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute testing time\n",
    "t_testing = t_after - t_before\n",
    "print(\"Time to classify test set (seconds): \", t_testing)\n",
    "\n",
    "## Verify that the predictions are the same\n",
    "print(\"Ball tree produces same predictions as above? \", np.array_equal(test_predictions, ball_tree_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KDTree\n",
    "\n",
    "## Build nearest neighbor structure on training data\n",
    "t_before = time.time()\n",
    "kd_tree = KDTree(train_data)\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute training time\n",
    "t_training = t_after - t_before\n",
    "print(\"Time to build data structure (seconds): \", t_training)\n",
    "\n",
    "## Get nearest neighbor predictions on testing data\n",
    "t_before = time.time()\n",
    "test_neighbors = np.squeeze(kd_tree.query(test_data, k=1, return_distance=False))\n",
    "kd_tree_predictions = train_labels[test_neighbors]\n",
    "t_after = time.time()\n",
    "\n",
    "## Compute testing time\n",
    "t_testing = t_after - t_before\n",
    "print(\"Time to classify test set (seconds): \", t_testing)\n",
    "\n",
    "## Verify that the predictions are the same\n",
    "print(\"KD tree produces same predictions as above? \", np.array_equal(test_predictions, kd_tree_predictions))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "153px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": false,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
